{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Metrics Library","text":"<p>Welcome to the Metrics Library! This package provides a suite of modelling metric functions with optional Gen AI generated descriptions.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Metric implementations: MSE, MAE, Accuracy, F1 Score, Log Loss.</li> <li>Gen AI description: Generate natural\u2011language explanations for metric results.</li> <li>Scalable design: Easy to extend with new metrics and prompts.</li> <li>Fully tested: 90%+ test coverage.</li> <li>Documentation: Hosted on GitHub Pages via MkDocs.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install metrics_library\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from metrics_library.metrics import MeanSquaredError\nfrom metrics_library.genai.description_generator import DescriptionGenerator\n\nmse = MeanSquaredError()\nresult = mse.calculate([3, -0.5, 2, 7], [2.5, 0.0, 2.0, 8.0])\n\ndesc = DescriptionGenerator().generate(result)\nprint(desc)\n</code></pre>"},{"location":"metrics/","title":"Metrics","text":"<p>This library provides the following metric implementations:</p>"},{"location":"metrics/#mean-squared-error-meansquarederror","title":"Mean Squared Error (<code>MeanSquaredError</code>)","text":"<p>Calculates the average of the squared differences between true and predicted values.</p> <pre><code>from metrics_library.metrics import MeanSquaredError\nmse = MeanSquaredError()\nresult = mse.calculate(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#mean-absolute-error-meanabsoluteerror","title":"Mean Absolute Error (<code>MeanAbsoluteError</code>)","text":"<p>Calculates the average absolute difference between true and predicted values.</p> <pre><code>from metrics_library.metrics import MeanAbsoluteError\nmae = MeanAbsoluteError()\nresult = mae.calculate(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#accuracy-accuracy","title":"Accuracy (<code>Accuracy</code>)","text":"<p>Proportion of correct predictions for classification tasks.</p> <pre><code>from metrics_library.metrics import Accuracy\nacc = Accuracy()\nresult = acc.calculate(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#f1-score-f1score","title":"F1 Score (<code>F1Score</code>)","text":"<p>Harmonic mean of precision and recall for binary classification.</p> <pre><code>from metrics_library.metrics import F1Score\nf1 = F1Score()\nresult = f1.calculate(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#log-loss-logloss","title":"Log Loss (<code>LogLoss</code>)","text":"<p>Binary cross\u2011entropy loss for classification probabilities.</p> <pre><code>from metrics_library.metrics import LogLoss\nlogloss = LogLoss()\nresult = logloss.calculate(y_true, y_pred_proba)\n</code></pre> <p>Each metric returns a <code>MetricResult</code> object containing the <code>value</code>, <code>name</code>, and optional <code>metadata</code>.</p>"},{"location":"reference/core/","title":"Core Module","text":""},{"location":"reference/core/#metrics_library.core.base","title":"base","text":"<p>Base classes for metrics library.</p> <p>This module defines the abstract base class <code>Metric</code> and the <code>MetricResult</code> dataclass that all metrics in the library should use.</p>"},{"location":"reference/core/#metrics_library.core.base.Metric","title":"Metric","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all metrics.</p> <p>All custom metrics should inherit from this class and implement the <code>calculate</code> method.</p> Source code in <code>src/metrics_library/core/base.py</code> <pre><code>class Metric(abc.ABC):\n    \"\"\"Abstract base class for all metrics.\n\n    All custom metrics should inherit from this class and implement the\n    `calculate` method.\n    \"\"\"\n\n    def __init__(self, name: str):\n        \"\"\"Initializes the Metric.\n\n        Args:\n            name: The name of the metric.\n        \"\"\"\n        self._name = name\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Returns the name of the metric.\"\"\"\n        return self._name\n\n    @abc.abstractmethod\n    def calculate(\n        self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n    ) -&gt; MetricResult:\n        \"\"\"Calculates the metric.\n\n        Args:\n            y_true: Ground truth (correct) target values.\n            y_pred: Estimated targets as returned by a classifier or regressor.\n\n        Returns:\n            A MetricResult object containing the calculated value.\n        \"\"\"\n        pass\n\n    def get_description(self, result: MetricResult) -&gt; str:\n        \"\"\"Generates a description for the metric result.\n\n        This method is intended to be overridden or used in conjunction with\n        the Gen AI components to produce a natural language description.\n\n        Args:\n            result: The result of the metric calculation.\n\n        Returns:\n            A string description of the result.\n        \"\"\"\n        return f\"The {self.name} is {result.value}.\"\n</code></pre>"},{"location":"reference/core/#metrics_library.core.base.Metric.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the name of the metric.</p>"},{"location":"reference/core/#metrics_library.core.base.Metric.__init__","title":"__init__","text":"<pre><code>__init__(name: str)\n</code></pre> <p>Initializes the Metric.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> required Source code in <code>src/metrics_library/core/base.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"Initializes the Metric.\n\n    Args:\n        name: The name of the metric.\n    \"\"\"\n    self._name = name\n</code></pre>"},{"location":"reference/core/#metrics_library.core.base.Metric.calculate","title":"calculate  <code>abstractmethod</code>","text":"<pre><code>calculate(\n    y_true: Union[ndarray, list],\n    y_pred: Union[ndarray, list],\n) -&gt; MetricResult\n</code></pre> <p>Calculates the metric.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Estimated targets as returned by a classifier or regressor.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>A MetricResult object containing the calculated value.</p> Source code in <code>src/metrics_library/core/base.py</code> <pre><code>@abc.abstractmethod\ndef calculate(\n    self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n) -&gt; MetricResult:\n    \"\"\"Calculates the metric.\n\n    Args:\n        y_true: Ground truth (correct) target values.\n        y_pred: Estimated targets as returned by a classifier or regressor.\n\n    Returns:\n        A MetricResult object containing the calculated value.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/core/#metrics_library.core.base.Metric.get_description","title":"get_description","text":"<pre><code>get_description(result: MetricResult) -&gt; str\n</code></pre> <p>Generates a description for the metric result.</p> <p>This method is intended to be overridden or used in conjunction with the Gen AI components to produce a natural language description.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>MetricResult</code> <p>The result of the metric calculation.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string description of the result.</p> Source code in <code>src/metrics_library/core/base.py</code> <pre><code>def get_description(self, result: MetricResult) -&gt; str:\n    \"\"\"Generates a description for the metric result.\n\n    This method is intended to be overridden or used in conjunction with\n    the Gen AI components to produce a natural language description.\n\n    Args:\n        result: The result of the metric calculation.\n\n    Returns:\n        A string description of the result.\n    \"\"\"\n    return f\"The {self.name} is {result.value}.\"\n</code></pre>"},{"location":"reference/core/#metrics_library.core.base.MetricResult","title":"MetricResult  <code>dataclass</code>","text":"<p>Container for the result of a metric calculation.</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>float</code> <p>The calculated metric value.</p> <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata about the calculation.</p> Source code in <code>src/metrics_library/core/base.py</code> <pre><code>@dataclass\nclass MetricResult:\n    \"\"\"Container for the result of a metric calculation.\n\n    Attributes:\n        value: The calculated metric value.\n        name: The name of the metric.\n        metadata: Additional metadata about the calculation.\n    \"\"\"\n\n    value: float\n    name: str\n    metadata: Optional[Dict[str, Any]] = None\n</code></pre>"},{"location":"reference/genai/","title":"GenAI Module","text":""},{"location":"reference/genai/#prompt-manager","title":"Prompt Manager","text":""},{"location":"reference/genai/#metrics_library.genai.prompts.PromptManager","title":"PromptManager","text":"<p>Manages prompts for Gen AI descriptions.</p> <p>This class handles loading prompts from a local YAML file or other sources.</p> Source code in <code>src/metrics_library/genai/prompts.py</code> <pre><code>class PromptManager:\n    \"\"\"Manages prompts for Gen AI descriptions.\n\n    This class handles loading prompts from a local YAML file or other sources.\n    \"\"\"\n\n    def __init__(self, prompt_file: Optional[str] = None):\n        \"\"\"Initializes the PromptManager.\n\n        Args:\n            prompt_file: Path to the YAML file containing prompts. If None,\n                defaults to the package's internal prompts.yaml.\n        \"\"\"\n        if prompt_file is None:\n            # Default to the package's data directory\n            base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n            prompt_file = os.path.join(base_dir, \"data\", \"prompts.yaml\")\n\n        self._prompts = self._load_prompts(prompt_file)\n\n    def _load_prompts(self, prompt_file: str) -&gt; Dict[str, Any]:\n        \"\"\"Loads prompts from a YAML file.\n\n        Args:\n            prompt_file: Path to the YAML file.\n\n        Returns:\n            A dictionary containing the prompts.\n        \"\"\"\n        try:\n            with open(prompt_file, \"r\") as f:\n                return yaml.safe_load(f) or {}\n        except FileNotFoundError:\n            # Fallback or log warning\n            return {}\n\n    def get_template(self, metric_name: str) -&gt; Optional[str]:\n        \"\"\"Retrieves the description template for a given metric.\n\n        Args:\n            metric_name: The name of the metric (snake_case).\n\n        Returns:\n            The description template string, or None if not found.\n        \"\"\"\n        metric_data = self._prompts.get(\"metrics\", {}).get(metric_name)\n        if metric_data:\n            return metric_data.get(\"description_template\")\n        return None\n</code></pre>"},{"location":"reference/genai/#metrics_library.genai.prompts.PromptManager.__init__","title":"__init__","text":"<pre><code>__init__(prompt_file: Optional[str] = None)\n</code></pre> <p>Initializes the PromptManager.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_file</code> <code>Optional[str]</code> <p>Path to the YAML file containing prompts. If None, defaults to the package's internal prompts.yaml.</p> <code>None</code> Source code in <code>src/metrics_library/genai/prompts.py</code> <pre><code>def __init__(self, prompt_file: Optional[str] = None):\n    \"\"\"Initializes the PromptManager.\n\n    Args:\n        prompt_file: Path to the YAML file containing prompts. If None,\n            defaults to the package's internal prompts.yaml.\n    \"\"\"\n    if prompt_file is None:\n        # Default to the package's data directory\n        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n        prompt_file = os.path.join(base_dir, \"data\", \"prompts.yaml\")\n\n    self._prompts = self._load_prompts(prompt_file)\n</code></pre>"},{"location":"reference/genai/#metrics_library.genai.prompts.PromptManager.get_template","title":"get_template","text":"<pre><code>get_template(metric_name: str) -&gt; Optional[str]\n</code></pre> <p>Retrieves the description template for a given metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The name of the metric (snake_case).</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The description template string, or None if not found.</p> Source code in <code>src/metrics_library/genai/prompts.py</code> <pre><code>def get_template(self, metric_name: str) -&gt; Optional[str]:\n    \"\"\"Retrieves the description template for a given metric.\n\n    Args:\n        metric_name: The name of the metric (snake_case).\n\n    Returns:\n        The description template string, or None if not found.\n    \"\"\"\n    metric_data = self._prompts.get(\"metrics\", {}).get(metric_name)\n    if metric_data:\n        return metric_data.get(\"description_template\")\n    return None\n</code></pre>"},{"location":"reference/genai/#description-generator","title":"Description Generator","text":""},{"location":"reference/genai/#metrics_library.genai.description_generator.DescriptionGenerator","title":"DescriptionGenerator","text":"<p>Generates a natural\u2011language description for a metric result.</p> <p>The generator loads a prompt template for the metric name from the YAML prompt file (see <code>data/prompts.yaml</code>) and formats it with the <code>value</code> attribute of a :class:<code>MetricResult</code>. If no template is found, it falls back to a generic description.</p> Source code in <code>src/metrics_library/genai/description_generator.py</code> <pre><code>class DescriptionGenerator:\n    \"\"\"Generates a natural\u2011language description for a metric result.\n\n    The generator loads a prompt template for the metric name from the YAML\n    prompt file (see ``data/prompts.yaml``) and formats it with the ``value``\n    attribute of a :class:`MetricResult`. If no template is found, it falls\n    back to a generic description.\n    \"\"\"\n\n    def __init__(self, prompt_file: str | None = None):\n        self._prompt_manager = PromptManager(prompt_file=prompt_file)\n\n    def generate(self, result: MetricResult) -&gt; str:\n        \"\"\"Return a description string for *result*.\n\n        Args:\n            result: The :class:`MetricResult` containing the metric name and\n                calculated value.\n\n        Returns:\n            A human\u2011readable description.\n        \"\"\"\n        template = self._prompt_manager.get_template(result.name)\n        if template:\n            try:\n                return template.format(value=result.value)\n            except Exception:\n                # If formatting fails, fall back to generic description\n                pass\n        # Generic fallback\n        return f\"The {result.name} metric value is {result.value:.4f}.\"\n</code></pre>"},{"location":"reference/genai/#metrics_library.genai.description_generator.DescriptionGenerator.generate","title":"generate","text":"<pre><code>generate(result: MetricResult) -&gt; str\n</code></pre> <p>Return a description string for result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>MetricResult</code> <p>The :class:<code>MetricResult</code> containing the metric name and calculated value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A human\u2011readable description.</p> Source code in <code>src/metrics_library/genai/description_generator.py</code> <pre><code>def generate(self, result: MetricResult) -&gt; str:\n    \"\"\"Return a description string for *result*.\n\n    Args:\n        result: The :class:`MetricResult` containing the metric name and\n            calculated value.\n\n    Returns:\n        A human\u2011readable description.\n    \"\"\"\n    template = self._prompt_manager.get_template(result.name)\n    if template:\n        try:\n            return template.format(value=result.value)\n        except Exception:\n            # If formatting fails, fall back to generic description\n            pass\n    # Generic fallback\n    return f\"The {result.name} metric value is {result.value:.4f}.\"\n</code></pre>"},{"location":"reference/metrics/","title":"Metrics Module","text":""},{"location":"reference/metrics/#mean-squared-error","title":"Mean Squared Error","text":""},{"location":"reference/metrics/#metrics_library.metrics.mse.MeanSquaredError","title":"MeanSquaredError","text":"<p>               Bases: <code>Metric</code></p> <p>Mean Squared Error (MSE) metric.</p> <p>The MSE is defined as the average of the squared differences between the true values and the predicted values.</p> Example <p>from metrics_library.metrics import MeanSquaredError mse = MeanSquaredError() result = mse.calculate([3, -0.5, 2, 7], [2.5, 0.0, 2, 8]) result.value 0.375</p> Source code in <code>src/metrics_library/metrics/mse.py</code> <pre><code>class MeanSquaredError(Metric):\n    \"\"\"Mean Squared Error (MSE) metric.\n\n    The MSE is defined as the average of the squared differences between the\n    true values and the predicted values.\n\n    Example:\n        &gt;&gt;&gt; from metrics_library.metrics import MeanSquaredError\n        &gt;&gt;&gt; mse = MeanSquaredError()\n        &gt;&gt;&gt; result = mse.calculate([3, -0.5, 2, 7], [2.5, 0.0, 2, 8])\n        &gt;&gt;&gt; result.value\n        0.375\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the MeanSquaredError metric.\"\"\"\n        super().__init__(name=\"mean_squared_error\")\n\n    def calculate(\n        self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n    ) -&gt; MetricResult:\n        \"\"\"Calculate the Mean Squared Error.\n\n        Args:\n            y_true: Ground truth (correct) target values.\n            y_pred: Estimated targets as returned by a regressor.\n\n        Returns:\n            A MetricResult object containing the MSE value and metric name.\n\n        Raises:\n            ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n        \"\"\"\n        y_true_arr = np.asarray(y_true, dtype=float)\n        y_pred_arr = np.asarray(y_pred, dtype=float)\n        if y_true_arr.shape != y_pred_arr.shape:\n            raise ValueError(\n                \"y_true and y_pred must have the same shape for MSE calculation.\"\n            )\n        mse = float(np.mean((y_true_arr - y_pred_arr) ** 2))\n        return MetricResult(value=mse, name=self.name)\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.mse.MeanSquaredError.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the MeanSquaredError metric.</p> Source code in <code>src/metrics_library/metrics/mse.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the MeanSquaredError metric.\"\"\"\n    super().__init__(name=\"mean_squared_error\")\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.mse.MeanSquaredError.calculate","title":"calculate","text":"<pre><code>calculate(\n    y_true: Union[ndarray, list],\n    y_pred: Union[ndarray, list],\n) -&gt; MetricResult\n</code></pre> <p>Calculate the Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Estimated targets as returned by a regressor.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>A MetricResult object containing the MSE value and metric name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_true</code> and <code>y_pred</code> have mismatched shapes.</p> Source code in <code>src/metrics_library/metrics/mse.py</code> <pre><code>def calculate(\n    self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n) -&gt; MetricResult:\n    \"\"\"Calculate the Mean Squared Error.\n\n    Args:\n        y_true: Ground truth (correct) target values.\n        y_pred: Estimated targets as returned by a regressor.\n\n    Returns:\n        A MetricResult object containing the MSE value and metric name.\n\n    Raises:\n        ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n    \"\"\"\n    y_true_arr = np.asarray(y_true, dtype=float)\n    y_pred_arr = np.asarray(y_pred, dtype=float)\n    if y_true_arr.shape != y_pred_arr.shape:\n        raise ValueError(\n            \"y_true and y_pred must have the same shape for MSE calculation.\"\n        )\n    mse = float(np.mean((y_true_arr - y_pred_arr) ** 2))\n    return MetricResult(value=mse, name=self.name)\n</code></pre>"},{"location":"reference/metrics/#mean-absolute-error","title":"Mean Absolute Error","text":""},{"location":"reference/metrics/#metrics_library.metrics.mae.MeanAbsoluteError","title":"MeanAbsoluteError","text":"<p>               Bases: <code>Metric</code></p> <p>Mean Absolute Error (MAE) metric.</p> <p>MAE is the average of absolute differences between true and predicted values.</p> Example <p>from metrics_library.metrics import MeanAbsoluteError mae = MeanAbsoluteError() result = mae.calculate([3, -0.5, 2, 7], [2.5, 0.0, 2, 8]) result.value 0.5</p> Source code in <code>src/metrics_library/metrics/mae.py</code> <pre><code>class MeanAbsoluteError(Metric):\n    \"\"\"Mean Absolute Error (MAE) metric.\n\n    MAE is the average of absolute differences between true and predicted values.\n\n    Example:\n        &gt;&gt;&gt; from metrics_library.metrics import MeanAbsoluteError\n        &gt;&gt;&gt; mae = MeanAbsoluteError()\n        &gt;&gt;&gt; result = mae.calculate([3, -0.5, 2, 7], [2.5, 0.0, 2, 8])\n        &gt;&gt;&gt; result.value\n        0.5\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the MeanAbsoluteError metric.\"\"\"\n        super().__init__(name=\"mean_absolute_error\")\n\n    def calculate(\n        self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n    ) -&gt; MetricResult:\n        \"\"\"Calculate the Mean Absolute Error.\n\n        Args:\n            y_true: Ground truth (correct) target values.\n            y_pred: Estimated targets as returned by a regressor.\n\n        Returns:\n            A MetricResult object containing the MAE value and metric name.\n\n        Raises:\n            ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n        \"\"\"\n        y_true_arr = np.asarray(y_true, dtype=float)\n        y_pred_arr = np.asarray(y_pred, dtype=float)\n        if y_true_arr.shape != y_pred_arr.shape:\n            raise ValueError(\n                \"y_true and y_pred must have the same shape for MAE calculation.\"\n            )\n        mae = float(np.mean(np.abs(y_true_arr - y_pred_arr)))\n        return MetricResult(value=mae, name=self.name)\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.mae.MeanAbsoluteError.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the MeanAbsoluteError metric.</p> Source code in <code>src/metrics_library/metrics/mae.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the MeanAbsoluteError metric.\"\"\"\n    super().__init__(name=\"mean_absolute_error\")\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.mae.MeanAbsoluteError.calculate","title":"calculate","text":"<pre><code>calculate(\n    y_true: Union[ndarray, list],\n    y_pred: Union[ndarray, list],\n) -&gt; MetricResult\n</code></pre> <p>Calculate the Mean Absolute Error.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>Ground truth (correct) target values.</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Estimated targets as returned by a regressor.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>A MetricResult object containing the MAE value and metric name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_true</code> and <code>y_pred</code> have mismatched shapes.</p> Source code in <code>src/metrics_library/metrics/mae.py</code> <pre><code>def calculate(\n    self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n) -&gt; MetricResult:\n    \"\"\"Calculate the Mean Absolute Error.\n\n    Args:\n        y_true: Ground truth (correct) target values.\n        y_pred: Estimated targets as returned by a regressor.\n\n    Returns:\n        A MetricResult object containing the MAE value and metric name.\n\n    Raises:\n        ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n    \"\"\"\n    y_true_arr = np.asarray(y_true, dtype=float)\n    y_pred_arr = np.asarray(y_pred, dtype=float)\n    if y_true_arr.shape != y_pred_arr.shape:\n        raise ValueError(\n            \"y_true and y_pred must have the same shape for MAE calculation.\"\n        )\n    mae = float(np.mean(np.abs(y_true_arr - y_pred_arr)))\n    return MetricResult(value=mae, name=self.name)\n</code></pre>"},{"location":"reference/metrics/#accuracy","title":"Accuracy","text":""},{"location":"reference/metrics/#metrics_library.metrics.accuracy.Accuracy","title":"Accuracy","text":"<p>               Bases: <code>Metric</code></p> <p>Accuracy metric.</p> <p>Accuracy is the proportion of correct predictions.</p> Example <p>from metrics_library.metrics import Accuracy acc = Accuracy() result = acc.calculate([1, 0, 1, 1], [1, 0, 0, 1]) result.value 0.75</p> Source code in <code>src/metrics_library/metrics/accuracy.py</code> <pre><code>class Accuracy(Metric):\n    \"\"\"Accuracy metric.\n\n    Accuracy is the proportion of correct predictions.\n\n    Example:\n        &gt;&gt;&gt; from metrics_library.metrics import Accuracy\n        &gt;&gt;&gt; acc = Accuracy()\n        &gt;&gt;&gt; result = acc.calculate([1, 0, 1, 1], [1, 0, 0, 1])\n        &gt;&gt;&gt; result.value\n        0.75\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the Accuracy metric.\"\"\"\n        super().__init__(name=\"accuracy\")\n\n    def calculate(\n        self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n    ) -&gt; MetricResult:\n        \"\"\"Calculate the Accuracy.\n\n        Args:\n            y_true: Ground truth (correct) target labels.\n            y_pred: Estimated targets as returned by a classifier.\n\n        Returns:\n            A MetricResult object containing the accuracy value and metric name.\n\n        Raises:\n            ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n        \"\"\"\n        y_true_arr = np.asarray(y_true)\n        y_pred_arr = np.asarray(y_pred)\n        if y_true_arr.shape != y_pred_arr.shape:\n            raise ValueError(\n                \"y_true and y_pred must have the same shape for Accuracy calculation.\"\n            )\n        correct = np.sum(y_true_arr == y_pred_arr)\n        accuracy = float(correct) / y_true_arr.size\n        return MetricResult(value=accuracy, name=self.name)\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.accuracy.Accuracy.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the Accuracy metric.</p> Source code in <code>src/metrics_library/metrics/accuracy.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the Accuracy metric.\"\"\"\n    super().__init__(name=\"accuracy\")\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.accuracy.Accuracy.calculate","title":"calculate","text":"<pre><code>calculate(\n    y_true: Union[ndarray, list],\n    y_pred: Union[ndarray, list],\n) -&gt; MetricResult\n</code></pre> <p>Calculate the Accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>Ground truth (correct) target labels.</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Estimated targets as returned by a classifier.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>A MetricResult object containing the accuracy value and metric name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_true</code> and <code>y_pred</code> have mismatched shapes.</p> Source code in <code>src/metrics_library/metrics/accuracy.py</code> <pre><code>def calculate(\n    self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n) -&gt; MetricResult:\n    \"\"\"Calculate the Accuracy.\n\n    Args:\n        y_true: Ground truth (correct) target labels.\n        y_pred: Estimated targets as returned by a classifier.\n\n    Returns:\n        A MetricResult object containing the accuracy value and metric name.\n\n    Raises:\n        ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n    \"\"\"\n    y_true_arr = np.asarray(y_true)\n    y_pred_arr = np.asarray(y_pred)\n    if y_true_arr.shape != y_pred_arr.shape:\n        raise ValueError(\n            \"y_true and y_pred must have the same shape for Accuracy calculation.\"\n        )\n    correct = np.sum(y_true_arr == y_pred_arr)\n    accuracy = float(correct) / y_true_arr.size\n    return MetricResult(value=accuracy, name=self.name)\n</code></pre>"},{"location":"reference/metrics/#f1-score","title":"F1 Score","text":""},{"location":"reference/metrics/#metrics_library.metrics.f1_score.F1Score","title":"F1Score","text":"<p>               Bases: <code>Metric</code></p> <p>F1 Score metric.</p> <p>The F1 Score is the harmonic mean of precision and recall.</p> Example <p>from metrics_library.metrics import F1Score f1 = F1Score() result = f1.calculate([1, 0, 1, 1, 0, 0], [1, 0, 0, 1, 0, 1]) result.value 0.6666666666666666</p> Source code in <code>src/metrics_library/metrics/f1_score.py</code> <pre><code>class F1Score(Metric):\n    \"\"\"F1 Score metric.\n\n    The F1 Score is the harmonic mean of precision and recall.\n\n    Example:\n        &gt;&gt;&gt; from metrics_library.metrics import F1Score\n        &gt;&gt;&gt; f1 = F1Score()\n        &gt;&gt;&gt; result = f1.calculate([1, 0, 1, 1, 0, 0], [1, 0, 0, 1, 0, 1])\n        &gt;&gt;&gt; result.value\n        0.6666666666666666\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the F1Score metric.\"\"\"\n        super().__init__(name=\"f1_score\")\n\n    def _precision_recall(self, y_true: np.ndarray, y_pred: np.ndarray) -&gt; tuple[float, float]:\n        \"\"\"Calculate precision and recall for binary classification.\n\n        Args:\n            y_true: Ground truth binary labels.\n            y_pred: Predicted binary labels.\n\n        Returns:\n            A tuple of (precision, recall).\n        \"\"\"\n        # Assuming binary classification with labels 0 and 1\n        tp = np.sum((y_true == 1) &amp; (y_pred == 1))\n        fp = np.sum((y_true == 0) &amp; (y_pred == 1))\n        fn = np.sum((y_true == 1) &amp; (y_pred == 0))\n        precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0.0\n        recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0.0\n        return precision, recall\n\n    def calculate(\n        self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n    ) -&gt; MetricResult:\n        \"\"\"Calculate the F1 Score.\n\n        Args:\n            y_true: Ground truth (correct) binary target labels.\n            y_pred: Estimated binary targets as returned by a classifier.\n\n        Returns:\n            A MetricResult object containing the F1 score value and metric name.\n\n        Raises:\n            ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n        \"\"\"\n        y_true_arr = np.asarray(y_true)\n        y_pred_arr = np.asarray(y_pred)\n        if y_true_arr.shape != y_pred_arr.shape:\n            raise ValueError(\n                \"y_true and y_pred must have the same shape for F1Score calculation.\"\n            )\n        precision, recall = self._precision_recall(y_true_arr, y_pred_arr)\n        if precision + recall == 0:\n            f1 = 0.0\n        else:\n            f1 = 2 * precision * recall / (precision + recall)\n        return MetricResult(value=f1, name=self.name)\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.f1_score.F1Score.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the F1Score metric.</p> Source code in <code>src/metrics_library/metrics/f1_score.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the F1Score metric.\"\"\"\n    super().__init__(name=\"f1_score\")\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.f1_score.F1Score.calculate","title":"calculate","text":"<pre><code>calculate(\n    y_true: Union[ndarray, list],\n    y_pred: Union[ndarray, list],\n) -&gt; MetricResult\n</code></pre> <p>Calculate the F1 Score.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>Ground truth (correct) binary target labels.</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Estimated binary targets as returned by a classifier.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>A MetricResult object containing the F1 score value and metric name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_true</code> and <code>y_pred</code> have mismatched shapes.</p> Source code in <code>src/metrics_library/metrics/f1_score.py</code> <pre><code>def calculate(\n    self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n) -&gt; MetricResult:\n    \"\"\"Calculate the F1 Score.\n\n    Args:\n        y_true: Ground truth (correct) binary target labels.\n        y_pred: Estimated binary targets as returned by a classifier.\n\n    Returns:\n        A MetricResult object containing the F1 score value and metric name.\n\n    Raises:\n        ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n    \"\"\"\n    y_true_arr = np.asarray(y_true)\n    y_pred_arr = np.asarray(y_pred)\n    if y_true_arr.shape != y_pred_arr.shape:\n        raise ValueError(\n            \"y_true and y_pred must have the same shape for F1Score calculation.\"\n        )\n    precision, recall = self._precision_recall(y_true_arr, y_pred_arr)\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    return MetricResult(value=f1, name=self.name)\n</code></pre>"},{"location":"reference/metrics/#log-loss","title":"Log Loss","text":""},{"location":"reference/metrics/#metrics_library.metrics.log_loss.LogLoss","title":"LogLoss","text":"<p>               Bases: <code>Metric</code></p> <p>Log Loss metric.</p> <p>Computes the binary cross-entropy loss given true labels and predicted probabilities.</p> Example <p>from metrics_library.metrics import LogLoss logloss = LogLoss() result = logloss.calculate([1, 0, 1, 1, 0, 0], [0.9, 0.2, 0.8, 0.7, 0.1, 0.3]) result.value 0.21616...</p> Source code in <code>src/metrics_library/metrics/log_loss.py</code> <pre><code>class LogLoss(Metric):\n    \"\"\"Log Loss metric.\n\n    Computes the binary cross-entropy loss given true labels and predicted probabilities.\n\n    Example:\n        &gt;&gt;&gt; from metrics_library.metrics import LogLoss\n        &gt;&gt;&gt; logloss = LogLoss()\n        &gt;&gt;&gt; result = logloss.calculate([1, 0, 1, 1, 0, 0], [0.9, 0.2, 0.8, 0.7, 0.1, 0.3])\n        &gt;&gt;&gt; result.value\n        0.21616...\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the LogLoss metric.\"\"\"\n        super().__init__(name=\"log_loss\")\n\n    def calculate(\n        self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n    ) -&gt; MetricResult:\n        \"\"\"Calculate the Log Loss.\n\n        Args:\n            y_true: Ground truth (correct) binary target labels.\n            y_pred: Predicted probabilities for the positive class.\n\n        Returns:\n            A MetricResult object containing the log loss value and metric name.\n\n        Raises:\n            ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n        \"\"\"\n        y_true_arr = np.asarray(y_true, dtype=float)\n        y_pred_arr = np.asarray(y_pred, dtype=float)\n        if y_true_arr.shape != y_pred_arr.shape:\n            raise ValueError(\n                \"y_true and y_pred must have the same shape for LogLoss calculation.\"\n            )\n        # Clip predictions to avoid log(0)\n        eps = np.finfo(float).eps\n        y_pred_clipped = np.clip(y_pred_arr, eps, 1 - eps)\n        # Binary cross-entropy\n        loss = -np.mean(\n            y_true_arr * np.log(y_pred_clipped)\n            + (1 - y_true_arr) * np.log(1 - y_pred_clipped)\n        )\n        return MetricResult(value=float(loss), name=self.name)\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.log_loss.LogLoss.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> <p>Initialize the LogLoss metric.</p> Source code in <code>src/metrics_library/metrics/log_loss.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the LogLoss metric.\"\"\"\n    super().__init__(name=\"log_loss\")\n</code></pre>"},{"location":"reference/metrics/#metrics_library.metrics.log_loss.LogLoss.calculate","title":"calculate","text":"<pre><code>calculate(\n    y_true: Union[ndarray, list],\n    y_pred: Union[ndarray, list],\n) -&gt; MetricResult\n</code></pre> <p>Calculate the Log Loss.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Union[ndarray, list]</code> <p>Ground truth (correct) binary target labels.</p> required <code>y_pred</code> <code>Union[ndarray, list]</code> <p>Predicted probabilities for the positive class.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>A MetricResult object containing the log loss value and metric name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_true</code> and <code>y_pred</code> have mismatched shapes.</p> Source code in <code>src/metrics_library/metrics/log_loss.py</code> <pre><code>def calculate(\n    self, y_true: Union[np.ndarray, list], y_pred: Union[np.ndarray, list]\n) -&gt; MetricResult:\n    \"\"\"Calculate the Log Loss.\n\n    Args:\n        y_true: Ground truth (correct) binary target labels.\n        y_pred: Predicted probabilities for the positive class.\n\n    Returns:\n        A MetricResult object containing the log loss value and metric name.\n\n    Raises:\n        ValueError: If ``y_true`` and ``y_pred`` have mismatched shapes.\n    \"\"\"\n    y_true_arr = np.asarray(y_true, dtype=float)\n    y_pred_arr = np.asarray(y_pred, dtype=float)\n    if y_true_arr.shape != y_pred_arr.shape:\n        raise ValueError(\n            \"y_true and y_pred must have the same shape for LogLoss calculation.\"\n        )\n    # Clip predictions to avoid log(0)\n    eps = np.finfo(float).eps\n    y_pred_clipped = np.clip(y_pred_arr, eps, 1 - eps)\n    # Binary cross-entropy\n    loss = -np.mean(\n        y_true_arr * np.log(y_pred_clipped)\n        + (1 - y_true_arr) * np.log(1 - y_pred_clipped)\n    )\n    return MetricResult(value=float(loss), name=self.name)\n</code></pre>"}]}