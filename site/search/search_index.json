{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Metrics Library","text":"<p>Welcome to the Metrics Library! This package provides a suite of modelling metric functions with optional Gen AI generated descriptions.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Metric implementations: MSE, MAE, Accuracy, F1 Score, Log Loss.</li> <li>Gen AI description: Generate natural\u2011language explanations for metric results.</li> <li>Scalable design: Easy to extend with new metrics and prompts.</li> <li>Fully tested: 90%+ test coverage.</li> <li>Documentation: Hosted on GitHub Pages via MkDocs.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install metrics_library\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from metrics_library.metrics import MeanSquaredError\nfrom metrics_library.genai.description_generator import DescriptionGenerator\n\nmse = MeanSquaredError()\nresult = mse.calculate([3, -0.5, 2, 7], [2.5, 0.0, 2.0, 8.0])\n\ndesc = DescriptionGenerator().generate(result)\nprint(desc)\n</code></pre>"},{"location":"metrics/","title":"Metrics","text":"<p>This library provides the following metric implementations:</p>"},{"location":"metrics/#mean-squared-error-meansquarederror","title":"Mean Squared Error (<code>MeanSquaredError</code>)","text":"<p>Calculates the average of the squared differences between true and predicted values.</p> <pre><code>from metrics_library.metrics import MeanSquaredError\nmse = MeanSquaredError()\nresult = mse.calculate(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#mean-absolute-error-meanabsoluteerror","title":"Mean Absolute Error (<code>MeanAbsoluteError</code>)","text":"<p>Calculates the average absolute difference between true and predicted values.</p> <pre><code>from metrics_library.metrics import MeanAbsoluteError\nmae = MeanAbsoluteError()\nresult = mae.calculate(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#accuracy-accuracy","title":"Accuracy (<code>Accuracy</code>)","text":"<p>Proportion of correct predictions for classification tasks.</p> <pre><code>from metrics_library.metrics import Accuracy\nacc = Accuracy()\nresult = acc.calculate(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#f1-score-f1score","title":"F1 Score (<code>F1Score</code>)","text":"<p>Harmonic mean of precision and recall for binary classification.</p> <pre><code>from metrics_library.metrics import F1Score\nf1 = F1Score()\nresult = f1.calculate(y_true, y_pred)\n</code></pre>"},{"location":"metrics/#log-loss-logloss","title":"Log Loss (<code>LogLoss</code>)","text":"<p>Binary cross\u2011entropy loss for classification probabilities.</p> <pre><code>from metrics_library.metrics import LogLoss\nlogloss = LogLoss()\nresult = logloss.calculate(y_true, y_pred_proba)\n</code></pre> <p>Each metric returns a <code>MetricResult</code> object containing the <code>value</code>, <code>name</code>, and optional <code>metadata</code>.</p>"}]}